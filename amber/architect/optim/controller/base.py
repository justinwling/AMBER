from .... import backend as F
import os
import sys
import numpy as np
import h5py


class BaseController(object):
    """abstract class for controllers
    """

    def __init__(self, *args, **kwargs):
        # Abstract
        pass

    def __str__(self):
        return "AMBER Controller for architecture searching"

    def _create_weight(self):
        """Private method for creating tensors; called at initialization"""
        with F.variable_scope("create_weights"):
            with F.variable_scope("lstm", reuse=False):
                self.w_lstm = []
                for layer_id in range(self.lstm_num_layers):
                    with F.variable_scope("lstm_layer_{}".format(layer_id)):
                        w = F.create_parameter(
                            "w", [2 * self.lstm_size, 4 * self.lstm_size],
                            initializer='uniform'
                            )
                        self.w_lstm.append(w)

            # g_emb: initial controller hidden state tensor; to be learned
            self.g_emb = F.create_parameter("g_emb", [1, self.lstm_size], initializer='uniform')

            # w_emb: embedding for computational operations
            self.w_emb = {"start": []}

            with F.variable_scope("emb"):
                for layer_id in range(self.num_layers):
                    with F.variable_scope("layer_{}".format(layer_id)):
                        if self.share_embedding:
                            if layer_id not in self.share_embedding:
                                self.w_emb["start"].append(F.create_parameter(
                                    "w_start", [self.num_choices_per_layer[layer_id], self.lstm_size], initializer='uniform'))
                            else:
                                shared_id = self.share_embedding[layer_id]
                                assert shared_id < layer_id, \
                                    "You turned on `share_embedding`, but specified the layer %i " \
                                    "to be shared with layer %i, which is not built yet" % (layer_id, shared_id)
                                self.w_emb["start"].append(self.w_emb["start"][shared_id])

                        else:
                            self.w_emb["start"].append(F.create_parameter(
                                "w_start", [self.num_choices_per_layer[layer_id], self.lstm_size], initializer='uniform'))

            # w_soft: dictionary of tensors for transforming RNN hiddenstates to softmax classifier
            self.w_soft = {"start": []}
            with F.variable_scope("softmax"):
                for layer_id in range(self.num_layers):
                    if self.share_embedding:
                        if layer_id not in self.share_embedding:
                            with F.variable_scope("layer_{}".format(layer_id)):
                                self.w_soft["start"].append(F.create_parameter(
                                    name="w_start", shape=[self.lstm_size, self.num_choices_per_layer[layer_id]], initializer='uniform'))
                        else:
                            shared_id = self.share_embedding[layer_id]
                            assert shared_id < layer_id, \
                                "You turned on `share_embedding`, but specified the layer %i " \
                                "to be shared with layer %i, which is not built yet" % (layer_id, shared_id)
                            self.w_soft["start"].append(self.w_soft['start'][shared_id])
                    else:
                        with F.variable_scope("layer_{}".format(layer_id)):
                            self.w_soft["start"].append(F.create_parameter(
                                "w_start", [self.lstm_size, self.num_choices_per_layer[layer_id]], initializer='uniform'))

            #  w_attn_1/2, v_attn: for sampling skip connections
            if self.with_skip_connection:
                with F.variable_scope("attention"):
                    self.w_attn_1 = F.create_parameter("w_1", [self.lstm_size, self.lstm_size],initializer='uniform')
                    self.w_attn_2 = F.create_parameter("w_2", [self.lstm_size, self.lstm_size],initializer='uniform')
                    self.v_attn = F.create_parameter("v", [self.lstm_size, 1],initializer='uniform')
            else:
                self.w_attn_1 = None
                self.w_attn_2 = None
                self.v_attn = None

    def store(self, state, prob, action, reward, *args, **kwargs):
        """Store all necessary information and rewards for a given architecture

        This is the receiving method for controller to interact with manager by storing the rewards for a given architecture.
        The architecture and its probabilities can be generated by ``get_action()`` method.

        Parameters
        ----------
        state : list
            The state for which the action and probabilities are drawn.

        prob : list of ndarray
            A list of probabilities for each operation and skip connections.

        action : list
            A list of architecture tokens ordered as::

                [categorical_operation_0,
                categorical_operation_1, binary_skip_0,
                categorical_operation_2, binary_skip_0, binary_skip_1,
                ...]

        reward : float
            Reward for this architecture, as evaluated by ``amber.architect.manager``

        Returns
        -------
        None

        """
        self.buffer.store(state=state, prob=prob, action=action, reward=reward)
        return

    @staticmethod
    def remove_files(files, working_dir='.'):
        """Static method for removing files

        Parameters
        ----------
        files : list of str
            files to be removed

        working_dir : str
            filepath to working directory

        Returns
        -------
        None
        """
        for file in files:
            file = os.path.join(working_dir, file)
            if os.path.exists(file):
                os.remove(file)
    
    @property
    def search_space_size(self):
        input_blocks, output_blocks, num_layers, num_choices_per_layer = self.input_blocks, self.output_blocks, len(self.model_space), np.mean([len(layer) for layer in self.model_space])
        s = np.log10(num_choices_per_layer) * num_layers
        s += np.log10(2) * (num_layers-1)*num_layers/2
        s += np.log10(input_blocks) * num_layers + np.log10(output_blocks) * num_layers
        return s
    
    def train(self, *args, **kwargs):
        raise NotImplementedError("Abstract method.")